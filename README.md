# ğŸ“Š Transformation Task â€“ SSIS ETL Pipeline

This project builds a complete ETL pipeline using SSIS to process client-provided CSV files.
The data is extracted, cleaned, validated, and transformed using SSIS transformation tasks.
Finally, the transformed and quality-checked data is loaded into SQL Server.

---

## ğŸ“ Source Data Overview

Here, we have a total of **3 business dates**, and for each date there are **2 regional source files**.

**Regions:**

* East
* West

**Dates available in the data:**

* 10/02/2026
* 11/02/2026
* 12/02/2026

So, for each date, two CSV files are received (East and West).

---

## ğŸ“… Date: 2026-02-10

### East Source

<img width="484" height="128" alt="10-02-2026(East)" src="https://github.com/user-attachments/assets/43f15bad-261f-45be-9365-fd47fbbdf5b4" />

### West Source

<img width="490" height="122" alt="10-02-2026(West)" src="https://github.com/user-attachments/assets/e448627c-a583-4137-8f54-74205a627f1a" />

---

## ğŸ“… Date: 2026-02-11

### East Source

<img width="482" height="125" alt="11-02-2026(East)" src="https://github.com/user-attachments/assets/f4c90c34-25ef-43cf-bdb6-faa231ee4d7f" />

### West Source

<img width="465" height="123" alt="11-02-2026(West)" src="https://github.com/user-attachments/assets/0c5ea3b8-281b-48a0-bb95-aac41cc45359" />

---

## ğŸ“… Date: 2026-02-12

### East Source

<img width="466" height="121" alt="12-02-2026(East)" src="https://github.com/user-attachments/assets/b3133d1f-0204-4100-aec7-6e153b2c4dfc" />

### West Source

<img width="466" height="129" alt="12-02-2026(West)" src="https://github.com/user-attachments/assets/816ca476-585f-4a15-b1ff-0947800c1882" />

---

## ğŸ› ï¸ Project Objective

The main objective of this project is to build a **professional and reliable ETL pipeline** that:

* reads multiple CSV files received from the client,
* handles **two regions (East and West)**,
* processes data **date-wise**,
* performs required **data cleaning, validation, and transformations**, and
* loads the final, high-quality data into **SQL Server**.

---
### âœ… Client Requirements

* Daily sales data is received from **multiple CSV files** generated by different regional systems.
* Customer names must be **cleaned and standardized** because they are not consistent across files.
* Product codes must be **validated** before loading the data.
* **Invalid and incorrect records** must be identified and handled properly.
* Business requires:

  * **clean and trusted data**
  * **enriched data** for better reporting
  * **aggregated data** for analysis
  * **audit information** for tracking and validation
  * **final structured data loading** for reporting use

---

### âœ… Development Requirements

* Build a complete **SSIS ETL pipeline** to process multiple regional CSV source files.
* Implement **data cleansing and standardization logic** for customer names.
* Perform **product code validation** using the product master reference.
* Separate **valid and invalid records** using appropriate SSIS transformations.
* Apply required **data enrichment and aggregation logic**.
* Capture and store **audit and processing information**.
* Load the processed data into the following master tables:

  * **Dim_Customer_master_table (Customer_master_table)**

    * CustomerId
    * CustomerName
    * City

  * **Dim_Product_master_table (Product_master_table)**

    * ProductCode
    * ProductId
    * Category

## âœ… Solutions â€“ 1st Workflow

<img width="386" height="659" alt="Screenshot (905)" src="https://github.com/user-attachments/assets/dfee5d12-2673-4e72-a64c-befd55baec40" />

This is the complete workflow of this project.

---

### ğŸ”¹ Step 1 â€“ Initial Data Load and Append

For ease of processing, first of all, the complete source data is stored in the database.
Since all source CSV files have the **same schema**, all files are **appended into a single table**.

The first step of the solution is to retrieve all source files from the server using a **Foreach Loop Container**.

---

## ğŸ”§ Foreach Loop Container â€“ Configuration

<img width="738" height="700" alt="Foreach Configration" src="https://github.com/user-attachments/assets/42646e19-8612-423f-be5f-dda204f8feac" />

1. Go to the **Collection** tab and set the **Enumerator** to **Foreach File Enumerator**.
   *(The enumerator decides how files are iterated during execution. Foreach File Enumerator processes files one by one.)*

2. Provide the **folder path** where the source files are located.
   If the files are stored in multiple folders, provide only the **root folder path**.

3. Enable the **Traverse subfolders** option.

4. Go to the **Variable Mappings** tab and assign a variable with **Index = 0**.

This completes the configuration of the **Foreach Loop Container**.

---

## ğŸ” Data Flow Task â€“ *Load at SSMS*

<img width="327" height="318" alt="Screenshot (907)" src="https://github.com/user-attachments/assets/4d0aba38-76aa-4aee-a3c6-772a4aa2fb63" />

This is the main Data Flow Task named **â€œLoad at SSMSâ€**.

Inside this Data Flow Task:

1. **Flat File Source** is used to read the CSV file.
2. **OLE DB Destination** is used to store and append the CSV data into the SQL Server table.

The main configuration is used to provide a **dynamic file path** so that the package can automatically switch from one file to another during execution.

To configure the dynamic path:

* Open the **Flat File Connection Manager**.
* Press **F4**.
* Go to **Expressions**.
* Set the expression on **ConnectionString**.
* Assign the file path variable.

After this configuration, the data from all CSV files is loaded and appended successfully into the **SSMS server**.

<img width="438" height="502" alt="Screenshot (908)" src="https://github.com/user-attachments/assets/d6740e5f-cf9c-436e-a59b-a2309077a6a6" />
All **24 records** are stored successfully in the database.
The final table name is **`Final_Dataverse`**.

---

## ğŸ” Customer Name Standardization using Fuzzy Grouping

In the final table, some customer names are **not exactly the same** and contain **small spelling or formatting differences**.
To generate accurate and consistent customer names, **Fuzzy Grouping** is used.

**Fuzzy Grouping** is a technique that matches similar customer names with each other based on textual similarity.

---

### ğŸ”¹ Preparation for Fuzzy Grouping

Before applying Fuzzy Grouping, only the required columns are extracted from the final table.

From the **FinalDataverse** table, the following two columns are selected:

* `OrderId`
* `CustomerName`

These two columns are stored in a separate table named **`final_name`**.

This step is performed using an **Execute Process Task** in the workflow.

---

<img width="735" height="697" alt="Screenshot (909)" src="https://github.com/user-attachments/assets/72c8728d-0a6b-460c-befd-4ba4028dd030" />

---

### ğŸ”¹ Execute Process Task â€“ Configuration

The following SQL command is executed on the database through the **Execute Process Task**:

```sql
IF OBJECT_ID('dbo.final_name','U') IS NOT NULL
BEGIN
    DROP TABLE dbo.final_name;
END;

SELECT 
    OrderId,
    Customername
INTO dbo.final_name
FROM dbo.Final_Dataverse;
```

This step prepares the **`final_name`** table, which is later used as the input source for applying **Fuzzy Grouping** on customer names.


