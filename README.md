# ğŸ“Š Transformation Task â€“ SSIS ETL Pipeline

This project builds a complete ETL pipeline using SSIS to process client-provided CSV files.
The data is extracted, cleaned, validated, and transformed using SSIS transformation tasks.
Finally, the transformed and quality-checked data is loaded into SQL Server.

---

## ğŸ“ Source Data Overview

Here, we have a total of **3 business dates**, and for each date there are **2 regional source files**.

**Regions:**

* East
* West

**Dates available in the data:**

* 10/02/2026
* 11/02/2026
* 12/02/2026

So, for each date, two CSV files are received (East and West).

---

## ğŸ“… Date: 2026-02-10

### East Source

<img width="484" height="128" alt="10-02-2026(East)" src="https://github.com/user-attachments/assets/43f15bad-261f-45be-9365-fd47fbbdf5b4" />

### West Source

<img width="490" height="122" alt="10-02-2026(West)" src="https://github.com/user-attachments/assets/e448627c-a583-4137-8f54-74205a627f1a" />

---

## ğŸ“… Date: 2026-02-11

### East Source

<img width="482" height="125" alt="11-02-2026(East)" src="https://github.com/user-attachments/assets/f4c90c34-25ef-43cf-bdb6-faa231ee4d7f" />

### West Source

<img width="465" height="123" alt="11-02-2026(West)" src="https://github.com/user-attachments/assets/0c5ea3b8-281b-48a0-bb95-aac41cc45359" />

---

## ğŸ“… Date: 2026-02-12

### East Source

<img width="466" height="121" alt="12-02-2026(East)" src="https://github.com/user-attachments/assets/b3133d1f-0204-4100-aec7-6e153b2c4dfc" />

### West Source

<img width="466" height="129" alt="12-02-2026(West)" src="https://github.com/user-attachments/assets/816ca476-585f-4a15-b1ff-0947800c1882" />

---

## ğŸ› ï¸ Project Objective

The main objective of this project is to build a **professional and reliable ETL pipeline** that:

* reads multiple CSV files received from the client,
* handles **two regions (East and West)**,
* processes data **date-wise**,
* performs required **data cleaning, validation, and transformations**, and
* loads the final, high-quality data into **SQL Server**.

---
### âœ… Client Requirements

* Daily sales data is received from **multiple CSV files** generated by different regional systems.
* Customer names must be **cleaned and standardized** because they are not consistent across files.
* Product codes must be **validated** before loading the data.
* **Invalid and incorrect records** must be identified and handled properly.
* Business requires:

  * **clean and trusted data**
  * **enriched data** for better reporting
  * **aggregated data** for analysis
  * **audit information** for tracking and validation
  * **final structured data loading** for reporting use

---

### âœ… Development Requirements

* Build a complete **SSIS ETL pipeline** to process multiple regional CSV source files.
* Implement **data cleansing and standardization logic** for customer names.
* Perform **product code validation** using the product master reference.
* Separate **valid and invalid records** using appropriate SSIS transformations.
* Apply required **data enrichment and aggregation logic**.
* Capture and store **audit and processing information**.
* Load the processed data into the following master tables:

  * **Dim_Customer_master_table (Customer_master_table)**

    * CustomerId
    * CustomerName
    * City

  * **Dim_Product_master_table (Product_master_table)**

    * ProductCode
    * ProductId
    * Category

## âœ… Solutions â€“ 1st Workflow

<img width="386" height="659" alt="Screenshot (905)" src="https://github.com/user-attachments/assets/dfee5d12-2673-4e72-a64c-befd55baec40" />

This is the complete workflow of this project.

---

### ğŸ”¹ Step 1 â€“ Initial Data Load and Append

For ease of processing, first of all, the complete source data is stored in the database.
Since all source CSV files have the **same schema**, all files are **appended into a single table**.

The first step of the solution is to retrieve all source files from the server using a **Foreach Loop Container**.

---

## ğŸ”§ Foreach Loop Container â€“ Configuration

<img width="738" height="700" alt="Foreach Configration" src="https://github.com/user-attachments/assets/42646e19-8612-423f-be5f-dda204f8feac" />

1. Go to the **Collection** tab and set the **Enumerator** to **Foreach File Enumerator**.
   *(The enumerator decides how files are iterated during execution. Foreach File Enumerator processes files one by one.)*

2. Provide the **folder path** where the source files are located.
   If the files are stored in multiple folders, provide only the **root folder path**.

3. Enable the **Traverse subfolders** option.

4. Go to the **Variable Mappings** tab and assign a variable with **Index = 0**.

This completes the configuration of the **Foreach Loop Container**.

---

## ğŸ” Data Flow Task â€“ *Load at SSMS*

<img width="327" height="318" alt="Screenshot (907)" src="https://github.com/user-attachments/assets/4d0aba38-76aa-4aee-a3c6-772a4aa2fb63" />

This is the main Data Flow Task named **â€œLoad at SSMSâ€**.

Inside this Data Flow Task:

1. **Flat File Source** is used to read the CSV file.
2. **OLE DB Destination** is used to store and append the CSV data into the SQL Server table.

The main configuration is used to provide a **dynamic file path** so that the package can automatically switch from one file to another during execution.

To configure the dynamic path:

* Open the **Flat File Connection Manager**.
* Press **F4**.
* Go to **Expressions**.
* Set the expression on **ConnectionString**.
* Assign the file path variable.

After this configuration, the data from all CSV files is loaded and appended successfully into the **SSMS server**.

<img width="438" height="502" alt="Screenshot (908)" src="https://github.com/user-attachments/assets/d6740e5f-cf9c-436e-a59b-a2309077a6a6" />
All **24 records** are stored successfully in the database.
The final table name is **`Final_Dataverse`**.

---

## ğŸ” Customer Name Standardization using Fuzzy Grouping

In the final table, some customer names are **not exactly the same** and contain **small spelling or formatting differences**.
To generate accurate and consistent customer names, **Fuzzy Grouping** is used.

**Fuzzy Grouping** is a technique that matches similar customer names with each other based on textual similarity.

---

### ğŸ”¹ Preparation for Fuzzy Grouping

Before applying Fuzzy Grouping, only the required columns are extracted from the final table.

From the **FinalDataverse** table, the following two columns are selected:

* `OrderId`
* `CustomerName`

These two columns are stored in a separate table named **`final_name`**.

This step is performed using an **Execute Process Task** in the workflow.

---

<img width="735" height="697" alt="Screenshot (909)" src="https://github.com/user-attachments/assets/72c8728d-0a6b-460c-befd-4ba4028dd030" />

---

### ğŸ”¹ Execute Process Task â€“ Configuration

The following SQL command is executed on the database through the **Execute Process Task**:

```sql
IF OBJECT_ID('dbo.final_name','U') IS NOT NULL
BEGIN
    DROP TABLE dbo.final_name;
END;

SELECT 
    OrderId,
    Customername
INTO dbo.final_name
FROM dbo.Final_Dataverse;
```

This step prepares the **`final_name`** table, which is later used as the input source for applying **Fuzzy Grouping** on customer names.

## ğŸ” Customer Name Standardization â€“ Fuzzy Grouping Data Flow

<img width="212" height="345" alt="Screenshot (910)" src="https://github.com/user-attachments/assets/fe50fb2f-2d61-47dd-b989-f38bafd07e7a" />

This is the next Data Flow Task executed after the **Execute SQL Task**.
This data flow is used to perform **Fuzzy Grouping**, which helps to generate accurate and standardized customer names in the system.

---

### ğŸ”¹ Fuzzy Grouping Configuration

<img width="680" height="726" alt="Screenshot (912)" src="https://github.com/user-attachments/assets/e300a799-3468-47d7-b383-002a9c0c402e" />

Open the **Fuzzy Grouping** transformation and select the column on which Fuzzy Grouping needs to be applied.

In this project, Fuzzy Grouping is applied on the **CustomerName** column.

The alias output column name is set as **`Correct_Name`**.

---

### ğŸ”¹ Advanced Configuration

<img width="680" height="715" alt="Screenshot (911)" src="https://github.com/user-attachments/assets/147e8474-c7e2-40c5-a612-19aba2ba9180" />

In the next tab of the Fuzzy Grouping editor, the configuration is set as shown above.

This configuration controls the similarity matching behavior and grouping logic.

---

### ğŸ”¹ Load Standardized Output

<img width="618" height="513" alt="Screenshot (913)" src="https://github.com/user-attachments/assets/a7a3ec6c-c2f8-49af-b1ed-db09c8b4b804" />

After applying Fuzzy Grouping, the cleaned and standardized output is stored in the database table named **`Correct_Name`**.

This output contains:

* the standardized customer name (**Correct_Name**),
* the original customer name,
* the similarity score, and
* grouping information.

This final output represents the clean customer name dataset, which is used in the next stages of the data quality and reporting pipeline.

## ğŸ§¹ Clean Data â€“ Build Dimension Master Tables

<img width="1010" height="650" alt="Screenshot (915)" src="https://github.com/user-attachments/assets/8baba8a9-6470-47dc-a926-abe0c29d0624" />

After completing the **Fuzzy Grouping** step, the next workflow step is **Clean Data**.
In this step, the cleaned and corrected customer names are used to generate the following dimension tables:

* **Dim_Customer_Master**
* **Dim_Product_Master**

This workflow ensures that only standardized and validated records are used for dimensional modeling.

---

### ğŸ”¹ Data Flow â€“ Clean Data

This step is implemented as a **Data Flow Task** named **Clean Data**.

The main purpose of this data flow is to:

* use the **corrected customer names** generated from the fuzzy matching step
* combine them with the transactional data
* prepare clean and reliable data for dimension tables

Two tables are used:

* `Final_Dataverse` â€“ contains the main order and transaction data
* `Correct_Name` â€“ contains the fuzzed and corrected customer names

Both tables are joined to create a clean and unified dataset.

---

### ğŸ”¹ OLE DB Source Configuration

The **OLE DB Source** is configured as follows:

**Access Mode**

```
SQL Command
```

**SQL Query**

```sql
SELECT DISTINCT
       o.OrderId,
       o.OrderDate,
       c.Correct_Name,
       o.ProductCode,
       o.Quantity,
       o.Amount,
       c._Similarity_Customername
FROM Final_Dataverse o
JOIN Correct_Name c
     ON o.OrderId = c.OrderId
ORDER BY o.OrderDate;
```

---

### ğŸ”¹ Purpose of Using SQL Command

The SQL command is used instead of separate SSIS transformations because:

* the **JOIN operation is executed directly in the database**
* it reduces the amount of data loaded into the SSIS pipeline
* it provides a more **optimized and faster execution**
* it simplifies the data flow design

---
## ğŸ§© Clean Data â€“ Join, Convert and Distribute Data for Dimension Tables

### ğŸ”¹ Output of Join Between `Final_Dataverse` and `Correct_Name`

<img width="574" height="512" alt="Screenshot (917)" src="https://github.com/user-attachments/assets/01eb3778-5fd3-40e7-af89-210c62682ea3" />

This output is generated by joining two tables:

* `Final_Dataverse`
* `Correct_Name`

The join is performed to attach the **fuzz-corrected customer name** with the main transactional data.

---

### ğŸ”¹ Data Conversion Transformation

<img width="756" height="722" alt="Screenshot (918)" src="https://github.com/user-attachments/assets/17d00c16-971b-4acc-8beb-210fc8ad09c9" />

After the join, a **Data Conversion transformation** is applied.

The purpose of this transformation is to standardize column data types:

* **Amount**

  * Original data contains mixed types (some values as integers and some as decimals).
  * The entire column is converted to:

    * **four-byte signed integer (DT_I4)**

* **OrderDate**

  * Converted to:

    * **Date format (DT_DATE)**

This step ensures consistent data types before loading into dimension tables.

---

### ğŸ”¹ Multicast â€“ Distribute Clean Data

After data conversion, the dataset is mostly clean and in the correct format.
The same dataset is then distributed into three parallel flows using a **Multicast** transformation.

The three outputs are:

1. **Dim_Customer_Master**
2. **Raw File Source**
3. **Dim_Product_Master**

This allows the same cleaned data to be sent to multiple destinations simultaneously without re-reading the source.

---

### ğŸ”¹ Dim_Customer_Master Data Flow

<img width="240" height="243" alt="Screenshot (919)" src="https://github.com/user-attachments/assets/d1154c09-9cd5-425b-a3bc-8d0a1a3c2160" />

The **Dim_Customer_Master** flow is used to build the customer dimension table.

The columns created in this flow are:

* **CustomerID**

  * Generated automatically at the destination using:
  * `ID INT IDENTITY`
  * This creates an incremental surrogate key.

* **City**

  * Created using a **Derived Column** transformation.
  * Expression used:

```sql
LEFT(OrderId,1) == "E" ? "East" :
LEFT(OrderId,1) == "W" ? "West" :
"NotFound"
```

* **CustomerName**

  * Taken from the **Correct_Name** column produced by the Clean Data step.
## ğŸ—‚ï¸ Clean Data â€“ Raw File & Product Dimension Load

---

### ğŸ”¹ 2nd Output â€“ Raw File Destination

In this step, the complete cleaned dataset is written to a **Raw File Destination**.

The main purpose of storing the data in a raw file is:

* very **low storage overhead** compared to other formats
* **high-performance read/write**
* no need to use a database connection when the file is reused in another SSIS package

This makes the raw file ideal for reuse in downstream pipelines and staging scenarios.

<img width="1790" height="471" alt="Screenshot (920)" src="https://github.com/user-attachments/assets/528430dd-daff-4647-8bd9-42ecb01e21ab" />

This image shows the output generated by the Raw File Destination.

---

### ğŸ”¹ 3rd Output â€“ Dim_Product_Master Data Flow

<img width="287" height="156" alt="Screenshot (921)" src="https://github.com/user-attachments/assets/094ef4eb-f000-45a1-a086-7ac83b2a09b5" />

The third branch from the Multicast transformation loads data into the **Dim_Product_Master** table.

This dimension contains only three business-required attributes:

* **ProductId**
* **ProductCode**
* **Category**

---

### ğŸ”¹ Column Design

**1. ProductId**

* Generated automatically at the destination table using:

```
id INT IDENTITY
```

This acts as a surrogate key for the product dimension.

---

**2. ProductCode**

* Retrieved directly from the cleaned dataset produced in the **Clean Data** step.
* No additional transformation is applied.

---

**3. Category**

* Created using a **Derived Column** transformation.
* The category is assigned based on the product code using the following expression:

```sql
ProductCode == "P1001" ? "Electronics" :
ProductCode == "P1002" ? "Accessories" :
ProductCode == "P1003" ? "Office Supplies" :
ProductCode == "P1004" ? "Home Appliances" :
"Unknown"
```

---## ğŸ§¾ Final Data Flow â€“ Fact & Aggregation Layer

<img width="1183" height="676" alt="Screenshot (922)" src="https://github.com/user-attachments/assets/42ed8351-fc28-477c-8ccf-7b327d110ba6" />

After completing the **Clean Data** and **dimension load** steps, this final data flow builds the analytical layer of the warehouse and produces the following outputs:

### âœ… Final Outputs

* **Fact_Sales_Clean**
* **Sales_Reject**
* **Sales_Daily_Summary** (aggregation table)

---

## ğŸ¯ Business Rules for Record Validation

A record is considered **rejected** if **any** of the following conditions is true:

* `ProductId` **IS NULL**
* `CustomerId` **IS NULL**
* `_Similarity_Customername < 0.80`

All remaining records are considered valid and are loaded into the fact table.

---

## ğŸ”— Data Enrichment Using Merge Join

To evaluate the above conditions, the following data is required:

* **CustomerID** â†’ from `Dim_Customer_Master`
* **ProductID** â†’ from `Dim_Product_Master`
* **Similarity score** â†’ from the Clean Data output

<img width="921" height="300" alt="Screenshot (924)" src="https://github.com/user-attachments/assets/3fda7a58-153e-49ba-a60b-ba7c0da21f08" />

Because the lookup data is coming from multiple tables, **Merge Join** transformations are used.

> âš ï¸ In SSIS, inputs to a Merge Join must be sorted.
> Therefore, **Sort transformations** are applied before every merge.

---

### ğŸ”¹ Step 1 â€“ Merge with Product Dimension

* Clean data is sorted by **ProductCode**
* `Dim_Product_Master` is sorted by **ProductCode**
* Both datasets are merged using **ProductCode** as the common key

This produces an intermediate dataset enriched with **ProductId**.

---

### ğŸ”¹ Step 2 â€“ Merge with Customer Dimension

<img width="422" height="375" alt="Screenshot (925)" src="https://github.com/user-attachments/assets/a0970f23-d889-40ed-9a07-7173d753199b" />

* The previous output is sorted by **Customer_Name**
* `Dim_Customer_Master` is sorted by **Customer_Name**
* Both datasets are merged using **Customer_Name** as the common key

This produces a fully enriched dataset containing:

* ProductId
* CustomerId
* Similarity score

---

## ğŸ”€ Conditional Split â€“ Accepted vs Rejected Records

<img width="715" height="718" alt="Screenshot (926)" src="https://github.com/user-attachments/assets/ac1c3061-947d-4cae-9804-d68e092ee1c3" />

A **Conditional Split** transformation is applied to separate valid and invalid records.

### âœ” Selected (Valid records)

Condition:

```
_Similarity_Customername > 0.80
```

### âŒ Rejected (Invalid records)

Condition:

```
ISNULL(ProductID) 
|| ISNULL(Customer_id) 
|| _Similarity_Customername < 0.80
```

---

## ğŸ§® Aggregation on Selected Records

In this dataset, the same customer (for example, *Jon Smith*) can place orders from multiple regions (East and West).
This causes duplicate logical entities when loading the fact table.

To handle this, an **Aggregate** transformation is applied on the selected branch using appropriate **GROUP BY** keys.

---

## ğŸ“¦ Fact Table Load

<img width="520" height="515" alt="Screenshot (927)" src="https://github.com/user-attachments/assets/592ce4ca-177b-4cb1-9563-97f199ae99eb" />

All validated and aggregated records are stored in:

```
Fact_Sales_Clean
```

**Result:**
âœ” 23 records loaded into the fact table.

---

## ğŸš« Rejected Records Load

<img width="598" height="103" alt="Screenshot (928)" src="https://github.com/user-attachments/assets/2c27af1a-db68-4b51-9a5f-97ee1cdebe60" />

All records failing the validation rules are stored in:

```
Sales_Reject
```

**Result:**
âŒ 1 record rejected.

---

## ğŸ“Š Sales Daily Summary â€“ Aggregation Layer

After loading the fact table, the selected stream is reused (via Multicast) to build the sales summary dataset.

<img width="860" height="428" alt="Screenshot (929)" src="https://github.com/user-attachments/assets/84e9fb82-e39e-4a47-b6c8-43d7bc62794e" />

The source for this step is:

```
Fact_Sales_Clean
```

---

### ğŸ”¹ 1. Regional Sales Metrics

<img width="534" height="117" alt="Screenshot (930)" src="https://github.com/user-attachments/assets/9133f8e4-3b40-41b1-827f-a3ae2db543e2" />

The following measures are calculated **per region**:

* Total_Sale
* Avg_Sale
* Max_Sale
* Min_Sale

---

### ğŸ”¹ 2. Ranking by Total Sales per Region

<img width="206" height="122" alt="Screenshot (931)" src="https://github.com/user-attachments/assets/6320008f-28f7-4bf2-a9f2-d98999a21cb1" />

A ranking is generated based on **total sales within each region**.

---

### ğŸ”¹ 3. Total Orders per Date

<img width="357" height="127" alt="Screenshot (932)" src="https://github.com/user-attachments/assets/1085bda9-127d-44ef-ac91-09e0bd1b0d91" />

The total number of orders is calculated for each **OrderDate**.

---

### ğŸ”¹ 4. Rollup-style Aggregation (Region, OrderDate)

<img width="309" height="235" alt="Screenshot (934)" src="https://github.com/user-attachments/assets/498df676-331a-4961-a94b-5b9391e24d2d" />

A rollup-style aggregation is implemented on:

```
(Region, OrderDate)
```

This produces hierarchical summaries similar to the SQL `ROLLUP` operator.

---

## ğŸ—„ï¸ Final Summary Table

All aggregation outputs are stored in:

```
Sales_Daily_Summary
```

---

## âœ… Final Result

This final data flow completes the ETL pipeline by:

* enriching clean data with product and customer surrogate keys
* validating records using business rules and similarity thresholds
* isolating rejected records for audit and quality checks
* building a clean, deduplicated fact table
* generating multiple analytical aggregations for reporting

This completes the end-to-end **clean, validate, enrich and aggregate** workflow of the project.

   
